{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9138c859-c0b1-4462-9f63-dd6cc60e7086",
   "metadata": {},
   "source": [
    "## Categorical Evaluation\n",
    "\n",
    "Produce Tables 8 and 9; agreement between human annotators and GPT for protagonist, antagonist, valence, and protagonist type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a6f90a-8457-4b93-ba2b-c5c5e2d9e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from typing import Set, Callabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050dba86-b97f-4bd7-9b76-b74bf278ddbb",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b227c8e-9eb9-4a97-ac33-e86e0b3fb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"../data/validation/moral_annotations\"\n",
    "\n",
    "# Load the response data\n",
    "df_eng = pd.read_csv(os.path.join(results_dir, \"human_responses_english.csv\"), index_col='index').fillna(\"None\")\n",
    "df_gpt_eng = pd.read_csv(os.path.join(results_dir, \"gpt_responses_english.csv\"), index_col='index').fillna(\"None\")\n",
    "df_chin = pd.read_csv(os.path.join(results_dir, \"human_responses_mandarin.csv\"), index_col='index').fillna(\"None\")\n",
    "df_gpt_chin = pd.read_csv(os.path.join(results_dir, \"gpt_responses_mandarin.csv\"), index_col='index').fillna(\"None\")\n",
    "\n",
    "# Get the relevant columns for the given category\n",
    "human_eng_annotators, human_chin_annotators = ['AL', 'AS', 'AS2', 'AZ', 'EA', 'NW'], ['JP', 'VX', 'YN', 'JY']\n",
    "gpt_eng_annotators, gpt_chin_annotators = ['0', '1', '2', '3', '4', '5'], ['0', '1', '2', '3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35837a6c-8668-4dbc-807b-a1c8c1ed54a4",
   "metadata": {},
   "source": [
    "## 1) Protagonist/Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2faa18-6369-4f98-b3aa-1ed6a6c9b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic list of synonyms in the validation dataset of protagonists/antagonists\n",
    "name_equivalences = {\n",
    "    \"chinese government\": 'china',\n",
    "    'government of china': 'china',\n",
    "    'japanese government': 'japan',\n",
    "    'government of japan': 'japan',\n",
    "    'european union government': 'eu',\n",
    "    'us': 'the united states',\n",
    "    'the protagonist of this story is the mouse': 'mouse'\n",
    "}\n",
    "\n",
    "# Clean names (remove accents and get rid of articles at the start of the name)\n",
    "def clean_word(word: str) -> str:\n",
    "    if \"the \" == word[0:4]:\n",
    "        word = word[4:]\n",
    "    elif \"a \" == word[0:2]:\n",
    "        word = word[2:]\n",
    "    elif \"an \" == word[0:3]:\n",
    "        word = word[3:]\n",
    "    word = word.replace(\"-\", \" \")\n",
    "    word = unidecode(word)    # remove accents\n",
    "    return name_equivalences.get(word, word)\n",
    "\n",
    "# Create a dictionary of equivalent names from the given list of names\n",
    "# Names are considered equivalent if one contains the other. While this is a bit simplistic, \n",
    "# it's a generally suitable heuristic for the names in the validation dataset\n",
    "def get_equivalence_classes(names_list: list[str]) -> dict[str:str]:\n",
    "    names_list = sorted(names_list, key=lambda x: len(x))\n",
    "    equiv_classes = dict()\n",
    "    for i in range(len(names_list)):\n",
    "        word1_orig = names_list[i]\n",
    "        word1 = clean_word(word1_orig)\n",
    "        for j in range(i+1, len(names_list)):\n",
    "            word2 = clean_word(names_list[j])\n",
    "            if len(word1) == len(word2):\n",
    "                continue\n",
    "            if word1 in word2:\n",
    "                equiv_classes[word1_orig] = word2\n",
    "        if word1_orig not in equiv_classes:\n",
    "            equiv_classes[word1_orig] = word1\n",
    "    return equiv_classes\n",
    "\n",
    "# Get the agreement breakdowns (agreement with majority, agreement with any, no agreement, average majority vote) of all predicted answers\n",
    "# relative to given reference answers. \n",
    "# Returns a torch matrix of size (n_references, 4, n_predictions) where output[:, :, i] is the agreement matrix of the references compared \n",
    "# to prediction i. Note this agreement matrix has values corresponding to \"agreement with majority\", \"agreement with any\", \"no agreement\", \n",
    "# \"average majority vote\" (all represented as ints or floats) in that order.\n",
    "# This function applies a scheme to equate answers together and uses that when determining agreement.\n",
    "def get_agreement_matrix(df_refs: pd.DataFrame, df_preds: pd.DataFrame) -> torch.Tensor:\n",
    "    n_examples = df_refs.shape[0]\n",
    "    n_predictions = df_preds.shape[1]\n",
    "    agreement_matrix = torch.zeros(n_examples, 4, n_predictions)\n",
    "    for i in range(n_predictions):\n",
    "        agreement_matrix[:, :, i] = get_agreement_breakdowns(df_refs, df_preds[df_preds.columns[i]])\n",
    "    return agreement_matrix\n",
    "\n",
    "# Get the agreement breakdown (agreement with majority, agreement with any, no agreement, average majority vote) for the given \n",
    "# predictions relative to given reference answers.\n",
    "# This function applies a scheme to equate answers together and uses that when determining agreement.\n",
    "def get_agreement_breakdowns(df_refs: pd.DataFrame, preds: pd.Series) -> torch.Tensor:\n",
    "    # normalize predicted answers\n",
    "    preds = preds.apply(lambda x: name_equivalences.get(x, x))\n",
    "\n",
    "    # compare the reference answers to the predicted ones\n",
    "    agreements = torch.zeros(df_refs.shape[0], 4)\n",
    "    for i, (_, row) in enumerate(df_refs.iterrows()):\n",
    "        # Get a mapping of equivalent names (need to add GPT's answer too so the equivalence class accounts for it)\n",
    "        equiv_classes = get_equivalence_classes(np.append(row.values, preds.iloc[i]))\n",
    "\n",
    "        # Map the answers to equivalent answers\n",
    "        ref_ans_counts = row.map(lambda x: equiv_classes.get(x, x)).value_counts()\n",
    "        pred_ans = equiv_classes.get(preds.iloc[i], preds.iloc[i])\n",
    "\n",
    "        # Check the agreement\n",
    "        if pred_ans not in ref_ans_counts.index:\n",
    "            agreed_with_majority, agreed_with_any, no_agreement = 0, 0, 1\n",
    "        elif ref_ans_counts.loc[pred_ans] == ref_ans_counts.max():\n",
    "            agreed_with_majority, agreed_with_any, no_agreement = 1, 1, 0\n",
    "        else:\n",
    "            agreed_with_majority, agreed_with_any, no_agreement = 0, 1, 0\n",
    "        agreements[i, :] = torch.Tensor([agreed_with_majority, agreed_with_any, no_agreement, ref_ans_counts.iloc[0]/ref_ans_counts.sum()])\n",
    "    return agreements\n",
    "\n",
    "# Get the list of genres corresponding to the given list of dataframes\n",
    "# Note that the index of the returned Series is not related to the index of the input dataframes\n",
    "def get_genre_list(df_list: list[pd.DataFrame]) -> pd.Series:\n",
    "    genres = []\n",
    "    for df in df_list:\n",
    "        genres += df.index.to_series().apply(lambda x: x.split(\"_\")[0]).values.tolist()\n",
    "    return pd.Series(genres)\n",
    "\n",
    "# Get the agreement breakdown by genre\n",
    "def get_agreement_by_genre(agreement_matrix: torch.Tensor, genre_series: pd.Series) -> pd.DataFrame:\n",
    "    genres = genre_series.unique()\n",
    "    genre_data = {genre:dict() for genre in genres}\n",
    "    for genre in genres:\n",
    "        genre_idxs = genre_series[genre_series == genre].index.to_list()\n",
    "        agreement_values = agreement_matrix[genre_idxs, :, :].mean(2).mean(0)\n",
    "        genre_data[genre] = {\n",
    "            'agree_with_majority': agreement_values[0].item(),\n",
    "            'any_agreement': agreement_values[1].item(),\n",
    "            'no_agreement': agreement_values[2].item(),\n",
    "            'avg_human_popular_vote': agreement_values[3].item()\n",
    "        }\n",
    "    return pd.DataFrame(genre_data).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bc396-b60f-49f8-a0d9-abe9266a9a20",
   "metadata": {},
   "source": [
    "### a) Protagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99925262-fdcb-47c1-b73b-4f30d09a8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protagonist stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree_with_majority</th>\n",
       "      <th>any_agreement</th>\n",
       "      <th>no_agreement</th>\n",
       "      <th>avg_human_popular_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>95.31</td>\n",
       "      <td>95.31</td>\n",
       "      <td>4.69</td>\n",
       "      <td>94.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folktale</th>\n",
       "      <td>81.25</td>\n",
       "      <td>95.31</td>\n",
       "      <td>4.69</td>\n",
       "      <td>78.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies-TV</th>\n",
       "      <td>84.38</td>\n",
       "      <td>85.94</td>\n",
       "      <td>14.06</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td>46.09</td>\n",
       "      <td>61.33</td>\n",
       "      <td>38.67</td>\n",
       "      <td>63.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reddit</th>\n",
       "      <td>87.50</td>\n",
       "      <td>93.75</td>\n",
       "      <td>6.25</td>\n",
       "      <td>91.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agree_with_majority  any_agreement  no_agreement  \\\n",
       "Book                     95.31          95.31          4.69   \n",
       "Folktale                 81.25          95.31          4.69   \n",
       "Movies-TV                84.38          85.94         14.06   \n",
       "News                     46.09          61.33         38.67   \n",
       "Reddit                   87.50          93.75          6.25   \n",
       "\n",
       "           avg_human_popular_vote  \n",
       "Book                        94.79  \n",
       "Folktale                    78.65  \n",
       "Movies-TV                   84.37  \n",
       "News                        63.41  \n",
       "Reddit                      91.67  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cate = 'protagonist'\n",
    "\n",
    "# Get the relevant columns\n",
    "human_eng_cols = [f\"{ann}_{cate}\" for ann in human_eng_annotators]\n",
    "human_chin_cols = [f\"{ann}_{cate}\" for ann in human_chin_annotators]\n",
    "gpt_eng_cols = [f\"{ann}_{cate}\" for ann in gpt_eng_annotators]\n",
    "gpt_chin_cols = [f\"{ann}_{cate}\" for ann in gpt_chin_annotators]\n",
    "\n",
    "num_agreement_cates = 4   # 4 agreement categories: agree_with_majority, any_agreement, no_agreement, avg_human_popular_vote\n",
    "num_gpt_comparisons = np.min([len(gpt_eng_cols), len(gpt_chin_cols)])\n",
    "n = df_eng.shape[0] + df_chin.shape[0]\n",
    "\n",
    "# Get the agreement matrices for each language\n",
    "agreement_matrix_eng = get_agreement_matrix(df_eng[human_eng_cols], df_gpt_eng[gpt_eng_cols])\n",
    "agreement_matrix_chin = get_agreement_matrix(df_chin[human_chin_cols], df_gpt_chin[gpt_chin_cols])\n",
    "\n",
    "# Combine the agreement matrices\n",
    "agreement_matrix = torch.cat([agreement_matrix_eng[:, :, :num_gpt_comparisons], agreement_matrix_chin[:, :, :num_gpt_comparisons]], axis=0)\n",
    "\n",
    "# Compute the agreement by genre\n",
    "genre_series = get_genre_list([df_eng[human_eng_cols], df_chin[human_chin_cols]])\n",
    "genre_agreement = get_agreement_by_genre(agreement_matrix, genre_series).round(4)*100\n",
    "\n",
    "print(f\"{cate}\".capitalize(), \"stats\")\n",
    "display(genre_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480a424-c88e-4390-b519-2fa28b4f591d",
   "metadata": {},
   "source": [
    "### b) Antagonist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "124a69c2-1cf6-4526-a920-470cd47e4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antagonist stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree_with_majority</th>\n",
       "      <th>any_agreement</th>\n",
       "      <th>no_agreement</th>\n",
       "      <th>avg_human_popular_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>56.25</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folktale</th>\n",
       "      <td>77.34</td>\n",
       "      <td>99.22</td>\n",
       "      <td>0.78</td>\n",
       "      <td>72.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies-TV</th>\n",
       "      <td>84.38</td>\n",
       "      <td>98.44</td>\n",
       "      <td>1.56</td>\n",
       "      <td>73.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td>67.19</td>\n",
       "      <td>96.88</td>\n",
       "      <td>3.12</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reddit</th>\n",
       "      <td>53.12</td>\n",
       "      <td>75.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agree_with_majority  any_agreement  no_agreement  \\\n",
       "Book                     56.25         100.00          0.00   \n",
       "Folktale                 77.34          99.22          0.78   \n",
       "Movies-TV                84.38          98.44          1.56   \n",
       "News                     67.19          96.88          3.12   \n",
       "Reddit                   53.12          75.00         25.00   \n",
       "\n",
       "           avg_human_popular_vote  \n",
       "Book                        54.17  \n",
       "Folktale                    72.92  \n",
       "Movies-TV                   73.96  \n",
       "News                        70.90  \n",
       "Reddit                      75.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cate = 'antagonist'\n",
    "\n",
    "# Get the relevant columns\n",
    "human_eng_cols = [f\"{ann}_{cate}\" for ann in human_eng_annotators]\n",
    "human_chin_cols = [f\"{ann}_{cate}\" for ann in human_chin_annotators]\n",
    "gpt_eng_cols = [f\"{ann}_{cate}\" for ann in gpt_eng_annotators]\n",
    "gpt_chin_cols = [f\"{ann}_{cate}\" for ann in gpt_chin_annotators]\n",
    "\n",
    "num_agreement_cates = 4   # 4 agreement categories: agree_with_majority, any_agreement, no_agreement, avg_human_popular_vote\n",
    "num_gpt_comparisons = np.min([len(gpt_eng_cols), len(gpt_chin_cols)])\n",
    "n = df_eng.shape[0] + df_chin.shape[0]\n",
    "\n",
    "# Get the agreement matrices for each language\n",
    "agreement_matrix_eng = get_agreement_matrix(df_eng[human_eng_cols], df_gpt_eng[gpt_eng_cols])\n",
    "agreement_matrix_chin = get_agreement_matrix(df_chin[human_chin_cols], df_gpt_chin[gpt_chin_cols])\n",
    "\n",
    "# Combine the agreement matrices\n",
    "agreement_matrix = torch.cat([agreement_matrix_eng[:, :, :num_gpt_comparisons], agreement_matrix_chin[:, :, :num_gpt_comparisons]], axis=0)\n",
    "\n",
    "# Compute the agreement by genre\n",
    "genre_series = get_genre_list([df_eng[human_eng_cols], df_chin[human_chin_cols]])\n",
    "genre_agreement = get_agreement_by_genre(agreement_matrix, genre_series).round(4)*100\n",
    "\n",
    "print(f\"{cate}\".capitalize(), \"stats\")\n",
    "display(genre_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c7067-e0a1-4b34-8478-ae568efbe739",
   "metadata": {},
   "source": [
    "## Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08f5ecbe-9d32-447f-b3e7-dd5953dcd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the statistics for each row in the dataframe (mode, median, mean, std)\n",
    "def get_row_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # get modes, medians and standard deviations\n",
    "    all_modes = df.mode(axis=1)\n",
    "    meds = df.median(axis=1).rename('median').to_frame()\n",
    "    means =  df.mean(axis=1).rename('mean').to_frame()\n",
    "    stds = df.std(axis=1).rename(\"std\")\n",
    "\n",
    "    # modes are returned as a dataframe with ties in separate columns, so merge those into a set\n",
    "    modes = []\n",
    "    for i, row in all_modes.iterrows():\n",
    "        modes.append(set(row[~row.isna()].astype(int)))\n",
    "    modes = pd.Series(modes, index=df.index, name='mode')\n",
    "\n",
    "    return pd.concat([modes, meds, means, stds], axis=1)\n",
    "\n",
    "# Compute the inter valence distributions between the two dataframes\n",
    "# Returns a numpy array of shape (df1.shape[0], 4, df2.shape[1]) where each (df1.shape[0], 4, i) slice gives the modes,\n",
    "# medians, means, and standard deviations (of each row) in the dataframe where column i of df2 has been added to df1\n",
    "def get_inter_valence_dists(df1: pd.DataFrame, df2: pd.DataFrame) -> np.array:\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        print(\"Error: shape mismatch\")\n",
    "        return\n",
    "\n",
    "    num_refs = df1.shape[0]\n",
    "    num_preds = df2.shape[1]\n",
    "    num_cols = 4       # mode, median, mean, std\n",
    "    \n",
    "    inter_vals = np.zeros((num_refs, num_cols, num_preds), dtype=object)\n",
    "    for i, (_, col) in enumerate(df2.items()):\n",
    "        df_tmp = pd.concat([df1, col], axis=1)\n",
    "        inter_vals[:, :, i] = get_row_stats(df_tmp).values\n",
    "        \n",
    "    return inter_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d5728d-86ef-46cd-993a-9de61122561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valence distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human-human</th>\n",
       "      <th>human-GPT</th>\n",
       "      <th>GPT-GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folktale</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies-TV</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reddit</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           human-human  human-GPT  GPT-GPT\n",
       "Book              0.77       0.77     0.11\n",
       "Folktale          0.75       0.86     0.16\n",
       "Movies-TV         0.79       0.83     0.21\n",
       "News              0.68       0.66     0.08\n",
       "Reddit            0.77       0.79     0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cate = 'valence'\n",
    "\n",
    "# Get the relevant columns\n",
    "human_eng_cols = [f\"{ann}_{cate}\" for ann in human_eng_annotators]\n",
    "human_chin_cols = [f\"{ann}_{cate}\" for ann in human_chin_annotators]\n",
    "gpt_eng_cols = [f\"{ann}_{cate}\" for ann in gpt_eng_annotators]\n",
    "gpt_chin_cols = [f\"{ann}_{cate}\" for ann in gpt_chin_annotators]\n",
    "\n",
    "# Get intra-dataframe stats\n",
    "human_valence_stats_eng = get_row_stats(df_eng[human_eng_cols])      # human\n",
    "human_valence_stats_chin = get_row_stats(df_chin[human_chin_cols])\n",
    "gpt_valence_stats_eng = get_row_stats(df_gpt_eng[gpt_eng_cols])       # gpt\n",
    "gpt_valence_stats_chin = get_row_stats(df_gpt_chin[gpt_chin_cols])\n",
    "\n",
    "# Combine English and Mandarin dists\n",
    "human_valence_stats = pd.concat([human_valence_stats_eng, human_valence_stats_chin], axis=0)\n",
    "gpt_valence_stats = pd.concat([gpt_valence_stats_eng, gpt_valence_stats_chin], axis=0)\n",
    "\n",
    "# Get inter-dataframe stats\n",
    "inter_valence_stats_eng = get_inter_valence_dists(df_eng[human_eng_cols], df_gpt_eng[gpt_eng_cols])\n",
    "inter_valence_stats_chin = get_inter_valence_dists(df_chin[human_chin_cols], df_gpt_chin[gpt_chin_cols])\n",
    "min_num_annotators = np.min([inter_valence_stats_eng.shape[2], inter_valence_stats_chin.shape[2]])\n",
    "inter_valence_stats = np.concatenate([inter_valence_stats_eng[:, :, :min_num_annotators], inter_valence_stats_chin[:, :, :min_num_annotators]], axis=0)\n",
    "\n",
    "# Compute the valence by genre\n",
    "genre_series = get_genre_list([df_eng[human_eng_cols], df_chin[human_chin_cols]])\n",
    "genre_dict = {genre:genre_series[genre_series == genre].index.to_list() for genre in genre_series.unique()}\n",
    "val_data = {'human-human': dict(), 'human-GPT': dict(), 'GPT-GPT': dict()}\n",
    "for genre in genre_dict:\n",
    "    genre_idx = genre_dict[genre]\n",
    "    val_data['human-human'][genre] = human_valence_stats.iloc[genre_idx]['std'].mean()\n",
    "    val_data['human-GPT'][genre] = inter_valence_stats[genre_idx][:, -1, :].mean(axis=1).mean()\n",
    "    val_data['GPT-GPT'][genre] = gpt_valence_stats.iloc[genre_idx]['std'].mean()\n",
    "val_df = pd.DataFrame(val_data).round(2)\n",
    "\n",
    "print(f\"{cate}\".capitalize(), 'distribution')\n",
    "display(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c548ed3-168e-40d2-a5c9-06689028094a",
   "metadata": {},
   "source": [
    "## Protagonist Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02636b-f2e7-44ff-a4eb-521527bfe75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the intra-dataframe scores between the two dataframes\n",
    "def compute_intra_scores(df: pd.DataFrame, score_func: Callable[pd.Series, pd.Series], **kwargs) -> torch.Tensor:\n",
    "    cols = df.columns\n",
    "    num_combos = scipy.special.comb(len(cols), 2).astype(int)\n",
    "    scores = torch.zeros(df.shape[0], num_combos)\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(len(cols)-1):\n",
    "        refs = df[cols[i]].tolist()\n",
    "        for j in range(i+1, len(cols)):\n",
    "            preds = df[cols[j]].tolist()\n",
    "            scores[:, k] = torch.Tensor(score_func(preds, refs, **kwargs))\n",
    "            k += 1\n",
    "    return scores.flatten()\n",
    "\n",
    "# Compute the inter-dataframe scores between the two dataframes\n",
    "def compute_inter_scores(\n",
    "        df1: pd.DataFrame, \n",
    "        df2: pd.DataFrame, \n",
    "        score_func: Callable[pd.Series, pd.Series], \n",
    "        **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "    if df1.shape[0] != df2.shape[0]:\n",
    "        print(\"Error: size mismatch\")\n",
    "        return\n",
    "    cols1, cols2 = df1.columns, df2.columns\n",
    "    num_combos = len(cols1)*len(cols2)\n",
    "    scores = torch.zeros(df1.shape[0], num_combos)\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len(cols1)):\n",
    "        # print(i)\n",
    "        refs = df1[cols1[i]].tolist()\n",
    "        for j in range(len(cols2)):\n",
    "            # print(\"\\t\", j)\n",
    "            preds = df2[cols2[j]].tolist()\n",
    "            scores[:, k] = torch.Tensor(score_func(preds, refs, **kwargs))\n",
    "            k += 1\n",
    "    return scores.flatten()\n",
    "\n",
    "# df_subset1 is all the human morals, df_subset2 is all the GPT morals\n",
    "def get_distributions(\n",
    "        df_subset1: pd.DataFrame, \n",
    "        df_subset2: pd.DataFrame, \n",
    "        score_funcs: dict[str:dict]\n",
    "    ) -> dict[str:dict[str:torch.Tensor]]:\n",
    "    data = {'1-1': dict(), '2-2': dict(), '1-2': dict()}\n",
    "    for score_name, score_func in score_funcs.items():\n",
    "        data['1-1'][score_name] = compute_intra_scores(df_subset1, score_func['func'], **score_func['kwargs'])\n",
    "        data['2-2'][score_name] = compute_intra_scores(df_subset2, score_func['func'], **score_func['kwargs'])\n",
    "        data['1-2'][score_name] = compute_inter_scores(df_subset1, df_subset2, score_func['func'], **score_func['kwargs'])\n",
    "    return data\n",
    "\n",
    "# Reshape distributions to their original size (they are currently flat)\n",
    "def reshape_dists(dists: dict, cols1: list, cols2: list) -> dict:\n",
    "    dists['1-1']['Jaccard'] = dists['1-1']['Jaccard'].reshape((-1, scipy.special.comb(len(cols1), 2).astype(int)))\n",
    "    dists['2-2']['Jaccard'] = dists['2-2']['Jaccard'].reshape((-1, scipy.special.comb(len(cols2), 2).astype(int)))\n",
    "    dists['1-2']['Jaccard'] = dists['1-2']['Jaccard'].reshape((-1, len(cols1)*len(cols2)))\n",
    "    return dists\n",
    "\n",
    "# Compute Jaccard similarity between set1 and set2\n",
    "def jaccard_similarity(set1: Set, set2: Set) -> float:\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection/union\n",
    "\n",
    "# Compute the Jaccard similarity between pairs of entries in series1 and series2\n",
    "# Returns list of floats (length = series1.shape[0] = series2.shape[0])\n",
    "def get_jaccard_sims_series(list1: pd.Series, list2: pd.Series) -> list[float]:\n",
    "    if len(list1) != len(list2):\n",
    "        print(\"Error: Size mismatch\")\n",
    "        return\n",
    "    jaccard_sims = []\n",
    "    for set1, set2 in zip(list1, list2):\n",
    "        jaccard_sims.append(jaccard_similarity(set1, set2))\n",
    "    return jaccard_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f44aa027-d76e-4b2d-9930-eb426c244ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protagonist Type distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human-human</th>\n",
       "      <th>human-GPT</th>\n",
       "      <th>GPT-GPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Book</th>\n",
       "      <td>57.92</td>\n",
       "      <td>54.40</td>\n",
       "      <td>91.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folktale</th>\n",
       "      <td>60.10</td>\n",
       "      <td>53.83</td>\n",
       "      <td>94.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies-TV</th>\n",
       "      <td>61.04</td>\n",
       "      <td>50.43</td>\n",
       "      <td>93.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td>46.77</td>\n",
       "      <td>44.77</td>\n",
       "      <td>90.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reddit</th>\n",
       "      <td>36.46</td>\n",
       "      <td>38.63</td>\n",
       "      <td>92.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           human-human  human-GPT  GPT-GPT\n",
       "Book             57.92      54.40    91.04\n",
       "Folktale         60.10      53.83    94.06\n",
       "Movies-TV        61.04      50.43    93.54\n",
       "News             46.77      44.77    90.31\n",
       "Reddit           36.46      38.63    92.08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cate = 'protagonist_type'\n",
    "\n",
    "# Name and specify the scoring functions (as well as there kwargs)\n",
    "score_funcs = {\n",
    "    'Jaccard': {'func': get_jaccard_sims_series, 'kwargs': {}}\n",
    "}\n",
    "\n",
    "# Get the relevant columns\n",
    "human_eng_cols = [f\"{ann}_{cate}\" for ann in human_eng_annotators]\n",
    "human_chin_cols = [f\"{ann}_{cate}\" for ann in human_chin_annotators]\n",
    "gpt_eng_cols = [f\"{ann}_{cate}\" for ann in gpt_eng_annotators]\n",
    "gpt_chin_cols = [f\"{ann}_{cate}\" for ann in gpt_chin_annotators]\n",
    "\n",
    "# Get English and Mandarin distibutions\n",
    "eng_dists = get_distributions(df_eng[human_eng_cols].map(lambda x: set(x.split(\", \"))), df_gpt_eng[gpt_eng_cols].map(lambda x: set(x.split(\", \"))), score_funcs)\n",
    "chin_dists = get_distributions(df_chin[human_chin_cols].map(lambda x: set(x.split(\", \"))), df_gpt_chin[gpt_chin_cols].map(lambda x: set(x.split(\", \"))), score_funcs)\n",
    "\n",
    "# Reshape the distributions\n",
    "eng_dists = reshape_dists(eng_dists, human_eng_cols, gpt_eng_cols)\n",
    "chin_dists = reshape_dists(chin_dists, human_chin_cols, gpt_chin_cols)\n",
    "\n",
    "# Combine distributions together\n",
    "total_dists = {'1-1': dict(), '2-2': dict(), '1-2': dict()}\n",
    "for dist in total_dists:\n",
    "    for score_name in score_funcs:\n",
    "        total_dists[dist][score_name] = torch.concat((eng_dists[dist][score_name].mean(axis=1), chin_dists[dist][score_name].mean(axis=1)))\n",
    "\n",
    "# Rename the index\n",
    "subset_names = ['human', 'GPT']\n",
    "idx_map = {\n",
    "    '1-1': f\"{subset_names[0]}-{subset_names[0]}\",\n",
    "    '2-2': f\"{subset_names[1]}-{subset_names[1]}\",\n",
    "    '1-2': f\"{subset_names[0]}-{subset_names[1]}\",\n",
    "}\n",
    "total_dists = {idx_map[idx]: total_dists[idx] for idx in ['1-1', '1-2', '2-2']}\n",
    "\n",
    "# Compute the Jaccard index by genre\n",
    "genre_series = get_genre_list([df_eng[human_eng_cols], df_chin[human_chin_cols]])\n",
    "genre_dict = {genre:genre_series[genre_series == genre].index.to_list() for genre in genre_series.unique()}\n",
    "genre_data = {col_type: dict() for col_type in total_dists}\n",
    "for col_type in total_dists:\n",
    "    col_scores = total_dists[col_type]['Jaccard']\n",
    "    genre_data[col_type].update({genre: col_scores[genre_dict[genre]].mean().item() for genre in genre_dict})\n",
    "# genre_data['p-value'] = {genre: scipy.stats.mannwhitneyu(\n",
    "#         total_dists['human-human']['Jaccard'][genre_dict[genre]],\n",
    "#         total_dists['human-GPT']['Jaccard'][genre_dict[genre]]\n",
    "#     ).pvalue for genre in genres}\n",
    "prot_type_by_genre = pd.DataFrame(genre_data)\n",
    "\n",
    "print(\"Protagonist Type distribution\")\n",
    "display(prot_type_by_genre.round(4)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
